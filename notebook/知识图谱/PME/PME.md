1. 解决了什么问题： 

	1. 缓解现有度量学习方法潜在的几何不灵活性
		1. 几何不灵活性：不满足三角不等式，保存一阶邻近度，没保存二阶邻近度，体现为：已经有线相连的距离小于没有线相连的距离
			1. 一阶：A-B
			2. 二阶：A-B，A-C，则B-C是二阶邻近
	2.  w.r.t关系分布严重不均匀
		1. 局部某个子网规模大，则这部分关系loss下降非常快，而别的子网没有什么变化

2. 怎么做的：

	1. 在单独的对象空间和关系空间而不是在公共空间中构建对象和关系嵌入

		1. 灵感来自TransR，投影到关系平面，计算两点间的欧几里得距离(L2)，并乘上相应的权重，目标函数同transe，[margin+正样例-负样例]+

			$$L=\sum_{r\in R}\sum_{(V_i,V_j)\in D_r}\sum_{(v_i,v_k)\notin D_r'}[m+f_r(v_i,v_j)^2-f_r(v_i,v_k)^2]_+$$

			$$f_r(v_i,v_j)=w_{ij}||M_rv_i-M_rv_j||$$

		2. 双向采样，保存关系的完整性，提高性能

			$$o=L=\sum_{r\in R}\sum_{(V_i,V_j)\in D_r}(\sum_{k=1}^KE_{v_k}\sim p_n(v)[m+f_r(v_i,v_j)^2-f_r(v_i,v_k)^2]_++\sum_{k=1}^KE_{v_k}\sim p_n(v)[m+f_r(v_i,v_j)^2-f_r(v_k,v_j)^2]_+)$$

	2. 基于loss感知的自适应采样方法

		1. 流程：
			1. 第一次梯度下降：初始化正抽样概率，与原始链路分布成正比，并按此抽取样本
			2.  第N次梯度下降：按正抽样概率来抽取样本
			3. 梯度下降，更新参数
			4. 根据loss计算正抽样概率
				1. 计算每个子网的局部loss
					1.  有几种类型的关系，就有几个子网
				2. 计算子网的局部loss在总loss中所占的概率
				3. 取[0,1]的随机数，看落在哪个子网的概率区间
					1. 效果：所占比例大的被选中的概率高。
				4. 增加所落的子网部分的正样本数
					1.  效果：这部分子网的loss会下降，下次梯度下降被选中的概率会降低
		2. 原理：如果子网规模大，梯度下降快，所占比例会降低，选中概率就会降低，正样本的加入会很慢；而更多选中的其他子网因为正样本的加入，loss也会很快下降

3. 和以前有什么区别

	1. 相较于TransR：借鉴了TransR，但是目的不同，是为了缓解几何不灵活性
	2. 加入了双向采样思想
	3.  根据loss动态改变正采样率

4. 数据集：https://www.yelp.com/dataset/challenge



